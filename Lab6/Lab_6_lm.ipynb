{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, TFAutoModelForCausalLM, AutoModelForQuestionAnswering, Wav2Vec2ForCTC\n",
    "from transformers import pipeline\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Read the documentation of Language modelling in the Transformers library.\n",
    "# 2. Download three Polish models from the Huggingface repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dkleczek/bert-base-polish-cased-v1 were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Downloading: 100%|██████████| 49.0/49.0 [00:00<00:00, 24.5kB/s]\n",
      "Downloading: 100%|██████████| 557/557 [00:00<00:00, 558kB/s]\n",
      "Downloading: 100%|██████████| 138k/138k [00:00<00:00, 434kB/s] \n",
      "Downloading: 100%|██████████| 232M/232M [00:21<00:00, 11.1MB/s]\n",
      "Downloading: 100%|██████████| 49.0/49.0 [00:00<00:00, 24.5kB/s]\n",
      "Downloading: 100%|██████████| 752/752 [00:00<00:00, 376kB/s]\n",
      "Downloading: 100%|██████████| 138k/138k [00:00<00:00, 429kB/s] \n",
      "Downloading: 100%|██████████| 394M/394M [00:36<00:00, 11.3MB/s]\n"
     ]
    }
   ],
   "source": [
    "# tokenizer_kleczek_uncased = AutoTokenizer.from_pretrained(\"dkleczek/bert-base-polish-uncased-v1\")\n",
    "# model_kleczek_uncased = AutoModelForMaskedLM.from_pretrained(\"dkleczek/bert-base-polish-uncased-v1\")\n",
    "tokenizer_kleczek_cased = AutoTokenizer.from_pretrained(\"dkleczek/bert-base-polish-cased-v1\")\n",
    "model_kleczek_cased = AutoModelForMaskedLM.from_pretrained(\"dkleczek/bert-base-polish-cased-v1\")\n",
    "# tokenizer_henryk_cased = AutoTokenizer.from_pretrained(\"henryk/bert-base-multilingual-cased-finetuned-polish-squad2\")\n",
    "# model_henryk_cased = AutoModelForQuestionAnswering.from_pretrained(\"henryk/bert-base-multilingual-cased-finetuned-polish-squad2\")\n",
    "# tokenizer_jonatasgrosman = AutoTokenizer.from_pretrained(\"jonatasgrosman/wav2vec2-large-xlsr-53-polish\")\n",
    "# model_jonatasgrosman = Wav2Vec2ForCTC.from_pretrained(\"jonatasgrosman/wav2vec2-large-xlsr-53-polish\") \n",
    "tokenizer_geotrend_distilbert = AutoTokenizer.from_pretrained(\"Geotrend/distilbert-base-pl-cased\")\n",
    "model_geotrend_distilbert = AutoModelForMaskedLM.from_pretrained(\"Geotrend/distilbert-base-pl-cased\")\n",
    "tokenizer_geotrend_bert = AutoTokenizer.from_pretrained(\"Geotrend/bert-base-pl-cased\")\n",
    "model_geotrend_bert = AutoModelForMaskedLM.from_pretrained(\"Geotrend/bert-base-pl-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Produce the predictions for the following sentences (use each model and check 5 predictions):\n",
    "\n",
    "- (M) Warszawa to największe [MASK].\n",
    "- (D) Te zabawki należą do [MASK].\n",
    "- (C) Policjant przygląda się [MASK].\n",
    "- (B) Na środku skrzyżowania widać [MASK].\n",
    "- (N) Właściciel samochodu widział złodzieja z [MASK].\n",
    "- (Ms) Prezydent z premierem rozmawiali wczoraj o [MASK].\n",
    "- (W) Witaj drogi [MASK].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_1 = {\n",
    "    'M': \"Warszawa to największe [MASK].\",\n",
    "    'D': \"Te zabawki należą do [MASK].\",\n",
    "    'C': \"Policjant przygląda się [MASK].\",\n",
    "    'B': \"Na środku skrzyżowania widać [MASK].\",\n",
    "    'N': \"Właściciel samochodu widział złodzieja z [MASK].\",\n",
    "    'Ms': \"Prezydent z premierem rozmawiali wczoraj o [MASK].\",\n",
    "    'W': \"Witaj drogi [MASK].\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_k_c = pipeline('fill-mask', model=model_kleczek_cased, tokenizer=tokenizer_kleczek_cased)\n",
    "nlp_g_d = pipeline('fill-mask', model=model_geotrend_distilbert, tokenizer=tokenizer_geotrend_distilbert)\n",
    "nlp_g_b = pipeline('fill-mask', model=model_geotrend_bert, tokenizer=tokenizer_geotrend_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'M': {'sentence': 'Warszawa to największe [MASK].',\n",
      "       'k_c': ['miasto', 'województwo', 'lotnisko', 'miasteczko', 'państwo'],\n",
      "       'g_d': ['miasto', 'miasta', 'Miasto', 'centrum', 'dzielnicy'],\n",
      "       'g_b': ['miasto', 'miasta', 'woj', 'Warszawa', 'miast']},\n",
      " 'D': {'sentence': 'Te zabawki należą do [MASK].',\n",
      "       'k_c': ['ciebie', 'mnie', 'nas', 'pana', 'niego'],\n",
      "       'g_d': ['klasyfikacji', 'gry', 'rodziny', 'grupy', 'zespołu'],\n",
      "       'g_b': ['tzw', 'pt', 'pl', 'odc', 'ok']},\n",
      " 'C': {'sentence': 'Policjant przygląda się [MASK].',\n",
      "       'k_c': ['temu', 'sprawie', 'im', 'wszystkiemu', 'panu'],\n",
      "       'g_d': ['przeciwko', 'LGBT', 'walki', 'nie', 'ludzi'],\n",
      "       'g_b': ['go', 'ok', 'się', 'pt', 'nie']},\n",
      " 'B': {'sentence': 'Na środku skrzyżowania widać [MASK].',\n",
      "       'k_c': ['rzekę', 'ulicę', 'drzewa', 'drogę', 'las'],\n",
      "       'g_d': ['wody', '##ały', 'miejsca', 'brak', '##ły'],\n",
      "       'g_b': ['św', 'ok', 'obraz', 'rok', 'kościół']},\n",
      " 'N': {'sentence': 'Właściciel samochodu widział złodzieja z [MASK].',\n",
      "       'k_c': ['bronią', 'tyłu', 'ulicy', 'bliska', 'zewnątrz'],\n",
      "       'g_d': ['Warszawy', 'pochodzenia', 'Niemiec', 'Łodzi', 'Rosji'],\n",
      "       'g_b': ['pt', 'tzw', 'ul', 'woj', 'kościoła']},\n",
      " 'Ms': {'sentence': 'Prezydent z premierem rozmawiali wczoraj o [MASK].',\n",
      "        'k_c': ['tym', 'Polsce', 'budżecie', 'ASF', 'ustawie'],\n",
      "        'g_d': ['prezydenta', 'referendum', 'władze', 'premiera', 'LGBT'],\n",
      "        'g_b': ['północy', 'tym', 'nim', 'co', 'św']},\n",
      " 'W': {'sentence': 'Witaj drogi [MASK].',\n",
      "       'k_c': ['chłopcze', 'przyjacielu', 'bracie', 'kolego', 'synu'],\n",
      "       'g_d': ['drogi', 'narodowej', 'polskiej', '##wej', 'wschodniej'],\n",
      "       'g_b': ['wyd', 'op', 'pt', 'ps', 'gen']}}\n"
     ]
    }
   ],
   "source": [
    "task_1_results = {}\n",
    "for id, sentence in texts_1.items():\n",
    "    if id not in task_1_results:\n",
    "        task_1_results[id] = {}\n",
    "    part_first, part_second = sentence.split(\"[MASK]\")\n",
    "    task_1_results[id][\"sentence\"] = sentence\n",
    "    task_1_results[id][\"k_c\"] = [word['token_str'] for word in nlp_k_c(f\"{part_first}{nlp_k_c.tokenizer.mask_token}{part_second}\")]\n",
    "    task_1_results[id][\"g_d\"] = [word['token_str'] for word in nlp_g_d(f\"{part_first}{nlp_g_d.tokenizer.mask_token}{part_second}\")]\n",
    "    task_1_results[id][\"g_b\"] = [word['token_str'] for word in nlp_g_b(f\"{part_first}{nlp_g_b.tokenizer.mask_token}{part_second}\")]\n",
    "pprint(task_1_results, sort_dicts=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Check the model predictions for the following sentences (using each model):\n",
    "\n",
    "- Gdybym wiedział wtedy dokładnie to, co wiem teraz, to bym się nie [MASK].\n",
    "- Gdybym wiedziała wtedy dokładnie to, co wiem teraz, to bym się nie [MASK].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_2 = {\n",
    "    'T1': \"Gdybym wiedział wtedy dokładnie to, co wiem teraz, to bym się nie [MASK].\",\n",
    "    'T2': \"Gdybym wiedziała wtedy dokładnie to, co wiem teraz, to bym się nie [MASK].\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'T1': {'sentence': 'Gdybym wiedział wtedy dokładnie to, co wiem teraz, to bym '\n",
      "                    'się nie [MASK].',\n",
      "        'k_c': ['zgodził', 'bał', 'dowiedział', 'pojawił', 'zabił'],\n",
      "        'g_d': ['było', 'stanie', 'udało', 'tylko', 'odbył'],\n",
      "        'g_b': ['##wiedział', 'było', 'stało', '##dził', 'stanie']},\n",
      " 'T2': {'sentence': 'Gdybym wiedziała wtedy dokładnie to, co wiem teraz, to '\n",
      "                    'bym się nie [MASK].',\n",
      "        'k_c': ['zgodziła', 'bała', 'dowiedziała', 'pojawiła', 'zabiła'],\n",
      "        'g_d': ['było', 'udało', 'stanie', 'tylko', 'ma'],\n",
      "        'g_b': ['było', '##wia', 'stało', '##śli', '##wiedział']}}\n"
     ]
    }
   ],
   "source": [
    "task_2_results = {}\n",
    "for id, sentence in texts_2.items():\n",
    "    if id not in task_2_results:\n",
    "        task_2_results[id] = {}\n",
    "    part_first, part_second = sentence.split(\"[MASK]\")\n",
    "    task_2_results[id][\"sentence\"] = sentence\n",
    "    task_2_results[id][\"k_c\"] = [word['token_str'] for word in nlp_k_c(f\"{part_first}{nlp_k_c.tokenizer.mask_token}{part_second}\")]\n",
    "    task_2_results[id][\"g_d\"] = [word['token_str'] for word in nlp_g_d(f\"{part_first}{nlp_g_d.tokenizer.mask_token}{part_second}\")]\n",
    "    task_2_results[id][\"g_b\"] = [word['token_str'] for word in nlp_g_b(f\"{part_first}{nlp_g_b.tokenizer.mask_token}{part_second}\")]\n",
    "pprint(task_2_results, sort_dicts=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Check the model predictions for the following sentences:\n",
    "\n",
    "- [MASK] wrze w temperaturze 100 stopni, a zamarza w temperaturze 0 stopni Celsjusza.\n",
    "- W wakacje odwiedziłem [MASK], który jest stolicą Islandii.\n",
    "- Informatyka na [MASK] należy do najlepszych kierunków w Polsce.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_3 = {\n",
    "    'T1': \"[MASK] wrze w temperaturze 100 stopni, a zamarza w temperaturze 0 stopni Celsjusza.\",\n",
    "    'T2': \"W wakacje odwiedziłem [MASK], który jest stolicą Islandii.\",\n",
    "    'T3': \"Informatyka na [MASK] należy do najlepszych kierunków w Polsce.\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'T1': {'sentence': '[MASK] wrze w temperaturze 100 stopni, a zamarza w '\n",
      "                    'temperaturze 0 stopni Celsjusza.',\n",
      "        'k_c': ['Woda', 'Mięso', 'Słońce', 'Nie', 'Ziemia'],\n",
      "        'g_d': ['Na', 'We', 'Od', 'Maja', 'Zmarł'],\n",
      "        'g_b': ['Jego', 'Za', 'Po', 'Nie', 'Ich']},\n",
      " 'T2': {'sentence': 'W wakacje odwiedziłem [MASK], który jest stolicą '\n",
      "                    'Islandii.',\n",
      "        'k_c': ['kraj', 'Cypr', 'Meksyk', 'Gibraltar', 'Wellington'],\n",
      "        'g_d': ['kraju', 'pochodzi', 'flag', 'wody', 'referendum'],\n",
      "        'g_b': ['Island', 'Reykjavík', 'Porto', 'miasto', 'Port']},\n",
      " 'T3': {'sentence': 'Informatyka na [MASK] należy do najlepszych kierunków w '\n",
      "                    'Polsce.',\n",
      "        'k_c': ['wsi', 'świecie', 'żywo', 'pewno', 'odległość'],\n",
      "        'g_d': ['stacji', 'Uniwersytecie', 'Ziemi', 'terenie', 'ulicy'],\n",
      "        'g_b': ['uczelni', 'rynku', 'internet', 'świecie', 'terenie']}}\n"
     ]
    }
   ],
   "source": [
    "task_3_results = {}\n",
    "for id, sentence in texts_3.items():\n",
    "    if id not in task_3_results:\n",
    "        task_3_results[id] = {}\n",
    "    part_first, part_second = sentence.split(\"[MASK]\")\n",
    "    task_3_results[id][\"sentence\"] = sentence\n",
    "    task_3_results[id][\"k_c\"] = [word['token_str'] for word in nlp_k_c(f\"{part_first}{nlp_k_c.tokenizer.mask_token}{part_second}\")]\n",
    "    task_3_results[id][\"g_d\"] = [word['token_str'] for word in nlp_g_d(f\"{part_first}{nlp_g_d.tokenizer.mask_token}{part_second}\")]\n",
    "    task_3_results[id][\"g_b\"] = [word['token_str'] for word in nlp_g_b(f\"{part_first}{nlp_g_b.tokenizer.mask_token}{part_second}\")]\n",
    "pprint(task_3_results, sort_dicts=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Answer the following questions:\n",
    "\n",
    "1. Which of the models produced the best results?\n",
    "    Najlepiej wypadł bert od Kłeczka. Generalnie dawał on najlepsze wyniki. \n",
    "\n",
    "2. Was any of the models able to capture Polish grammar?\n",
    "    Bert od Kłeczka w większosci przypadków robił to poprawnie. Czasami jeszcze distilbert od Geotrend.\n",
    "\n",
    "3. Was any of the models able to capture long-distant relationships between the words?\n",
    "    Raczej żaden w pełni, każdy po trochę.\n",
    "\n",
    "4. Was any of the models able to capture world knowledge?\n",
    "    Tylko bertowi od Kłeczka się udało.\n",
    "\n",
    "5. What are the most striking errors made by the models?\n",
    "    Większość generalnie próbuje generowac krótkie słowa, raczej spójniki i przyimki. Jak sa zdania złożone to nie rozumieją kontekstu.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "da09b68f613570449f88c91791aa0345a4974afa8206cbbcee6cca73a7d2ca93"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 32-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
